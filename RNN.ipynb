{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2375a615",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 什么是Embedding?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d960726",
   "metadata": {},
   "source": [
    "能够捕获数据含义的向量表示\n",
    "\n",
    "总的来说这个过程就是准确地将人类的语言翻译成计算机能够处理的数据，向量的维度越多，往往代表携带更多的信息，翻译地也会更准确\n",
    "\n",
    "有了嵌入向量之后，能够帮助我们计算词汇之间的语义相似度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4037f67b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 什么是词袋模型（Bag-of-Words）?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa097b7",
   "metadata": {},
   "source": [
    "词袋模型是整个文档根据词频，生成的向量表示\n",
    "\n",
    "存在的问题：\n",
    "\n",
    "- 丢失了文本的自然语义和词序，因为它只是把一堆词一股脑的装进了袋子里"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3998a546",
   "metadata": {},
   "source": [
    "### 什么是Word2Vec？为什么要有它？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cd7bd6",
   "metadata": {},
   "source": [
    "Word2Vec基于神经网络，使用大量的文本数据进行训练，能够捕获语义信息，并转换成计算机能够处理的向量，弥补了词袋模型的缺陷"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d455e88",
   "metadata": {},
   "source": [
    "### Word2Vec的神经网络是如何训练的？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c37adf",
   "metadata": {},
   "source": [
    "首先为每个词赋予一个固定长度的由随机数生成的嵌入向量，然后使用神经网络进行深度学习，通过不断地比较词与词之间的关联程度，更新嵌入向量，最终将语义的信息沉淀在嵌入向量中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2335e944",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Word2Vec有什么问题，为什么会出现RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a947c2",
   "metadata": {},
   "source": [
    "一个词具体的含义是由具体的上下文决定的。Word2Vec只能静态地描述一个词的含义。\n",
    "\n",
    "Word2Vec 有两个根本问题：\n",
    "\n",
    "1. **“死脑筋”**  \n",
    "   Word2Vec 给每个词只发一张**固定身份证**，比如“苹果”永远是 [0.2, 0.5, …]，不管它指水果还是指苹果公司。遇到多义词就傻眼了。\n",
    "\n",
    "2. **“不认顺序”**  \n",
    "   Word2Vec 只看词和词有没有经常一起出现，完全不管**谁先谁后**。它看不出“猫追老鼠”和“老鼠追猫”的区别，因为两句话说到底都是“猫、追、老鼠”这三个词。\n",
    "\n",
    "---\n",
    "\n",
    "**RNN 为什么出现？**\n",
    "\n",
    "因为我们想让模型**读句子像读故事**：  \n",
    "- 从左往右一个字一个字看，边看边记前文说了啥（这就是 RNN 的“记忆”）。  \n",
    "- 这样就知道“苹果”在“吃苹果”里是水果，在“苹果公司”里是企业。  \n",
    "- 也能明白“追”这个动词前面是“猫”还是“老鼠”，意思会天差地别。\n",
    "\n",
    "简单说：**Word2Vec 查词典，RNN 读故事**。读故事显然更能理解文本的先后顺序和上下文。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd31251",
   "metadata": {},
   "source": [
    "### 如何理解RNN中的 R，即循环"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb2989d",
   "metadata": {},
   "source": [
    "它会把每一次的输出当作下一次的输入\n",
    "\n",
    "把 RNN 想成“传话筒游戏”：\n",
    "\n",
    "**“循环” = 上一步的结果，偷偷塞回同一套网络，再算下一步**\n",
    "\n",
    "具体过程：  \n",
    "假设你要读句子 \"I love pizza\"：\n",
    "\n",
    "1. 先看 \"I\" → 网络算出一个结果（比如 [0.1, 0.2]），这就是**当前记忆**。  \n",
    "2. **关键动作**：把这个记忆**塞回自己肚子**（像贪吃蛇尾巴连着头），再读下一个词 \"love\"。  \n",
    "3. 读 \"love\" 时，网络不仅看到这个词，还看到上一步传来的记忆 [0.1, 0.2]，于是算出**新记忆** [0.3, 0.5]。  \n",
    "4. 再塞回自己，带着新记忆去读 \"pizza\"，得到最终记忆 [0.7, 0.9]。\n",
    "\n",
    "**为什么叫“循环”？**  \n",
    "因为网络结构**首尾相连**，像一个环：输出 → 输入 → 输出 → 输入……每一步都在用同一份参数、同一个“自己”，只是输入里混了上一步的记忆。\n",
    "\n",
    "**好处**  \n",
    "这样就能记住“前面说了啥”。如果不用循环，每个词都是孤立的，就像 Word2Vec 那样；有了循环，模型就知道 \"love\" 前面是 \"I\"，而不是 \"you\"。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
